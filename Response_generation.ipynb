{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Response_generation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMSmVGiH/F7djH3vAeFLh2Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Azhar-999-coder/conversational_bot/blob/master/Response_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivk54BuP4OSF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "3186526c-04db-4579-e634-3cd32df75bf2"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-07-17 14:21:34--  http://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.twitter.27B.zip [following]\n",
            "--2020-07-17 14:21:34--  https://nlp.stanford.edu/data/glove.twitter.27B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip [following]\n",
            "--2020-07-17 14:21:35--  http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1520408563 (1.4G) [application/zip]\n",
            "Saving to: ‘glove.twitter.27B.zip’\n",
            "\n",
            "glove.twitter.27B.z 100%[===================>]   1.42G  1.98MB/s    in 12m 0s  \n",
            "\n",
            "2020-07-17 14:33:35 (2.01 MB/s) - ‘glove.twitter.27B.zip’ saved [1520408563/1520408563]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BX16zAwFr_P",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "1b59fff9-9946-4b02-f4e6-902183f8f551"
      },
      "source": [
        "!unzip glove.twitter.27B.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  glove.twitter.27B.zip\n",
            "  inflating: glove.twitter.27B.25d.txt  \n",
            "  inflating: glove.twitter.27B.50d.txt  \n",
            "  inflating: glove.twitter.27B.100d.txt  \n",
            "  inflating: glove.twitter.27B.200d.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dzH0edVRF4Wy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Bidirectional, Concatenate, Permute, Dot, Input, LSTM, Multiply, GRU\n",
        "from tensorflow.keras.layers import RepeatVector, Dense, Activation, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OWaR6hJYKd4y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 100\n",
        "n_a = 64\n",
        "n_s = 128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjcmJYFL4C7O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "num_words=30000\n",
        "max_length=30\n",
        "Tx = max_length\n",
        "Ty = max_length\n",
        "\n",
        "path = 'Data/'\n",
        "\n",
        "dirlist = os.listdir(path)\n",
        "human_sentences=[]\n",
        "machine_sentences=[]\n",
        "for File in dirlist:\n",
        "    with open(path+\"/\"+File, 'r') as raw_lines:\n",
        "        lineList = []\n",
        "        while True:        \n",
        "            line = raw_lines.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            lineList.append(line)\n",
        "\n",
        "    for i in range(0, len(lineList)):\n",
        "        if(i%2)==0:\n",
        "            human_sentences.append(lineList[i])\n",
        "        else:\n",
        "            machine_sentences.append(lineList[i])    \n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "tokenizer.fit_on_texts(human_sentences)\n",
        "human_word_index = tokenizer.word_index\n",
        "human_reverse_word_index = {a:b for (b,a) in human_word_index.items()}\n",
        "tokenizerE=tokenizer\n",
        "\n",
        "tokenizer2 = tf.keras.preprocessing.text.Tokenizer(num_words=num_words, oov_token='<OOV>')\n",
        "    \n",
        "tokenizer2.fit_on_texts(machine_sentences)\n",
        "machine_word_index = tokenizer2.word_index\n",
        "machine_reverse_word_index = {a:b for (b,a) in machine_word_index.items()}\n",
        "\n",
        "human_sequences = tokenizer.texts_to_sequences(human_sentences)\n",
        "human_padded = tf.keras.preprocessing.sequence.pad_sequences(human_sequences, maxlen=max_length, padding='post', truncating=\"post\")\n",
        "\n",
        "machine_sequences = tokenizer2.texts_to_sequences(machine_sentences)\n",
        "machine_padded = tf.keras.preprocessing.sequence.pad_sequences(machine_sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
        "\n",
        "\n",
        "X = human_padded\n",
        "Y = machine_padded\n",
        "human_vocab = human_word_index\n",
        "reverse_human_vocab = human_reverse_word_index\n",
        "machine_vocab = machine_word_index\n",
        "reverse_machine_vocab = machine_reverse_word_index\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cJuFy19B_Yg-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#taking care of odd-even shit\n",
        "if X.shape[0]>Y.shape[0]:\n",
        "  X = np.delete(X, len(X)-1, axis=0)\n",
        "elif X.shape[0]<Y.shape[0]:\n",
        "  Y = Y.delete(Y, len(Y)-1, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gu8Ruv-eKIwo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating dataset\n",
        "dataset = tf.data.Dataset.from_tensor_slices((X,Y))\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K560oH2laxXh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5004b5d8-0609-4ec7-b140-28291d83ce65"
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 30)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wzz1-gtGa8yi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d5b8b4bd-82fd-4d2d-c9b6-4c69bec18b87"
      },
      "source": [
        "EMBEDDING_DIM = 200\n",
        "embeddings_index = {}\n",
        "f = open('glove.twitter.27B.200d.txt')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "\n",
        "print('Found %s word vectors.' % len(embeddings_index))\n",
        "\n",
        "embedding_matrix = np.zeros((len(lda_model.id2word) + 1, EMBEDDING_DIM)) #lda_model.id2word dicitonary\n",
        "for  i,word in lda_model.id2word.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "embedding_layer_keywords = Embedding(len(lda_model.id2word) + 1,\n",
        "                            EMBEDDING_DIM,\n",
        "                            weights=[embedding_matrix],\n",
        "                            #input_length=MAX_SEQ_LENGTH,\n",
        "                            #mask_zero = True,\n",
        "                            trainable=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1193514 word vectors.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7Y6wIjLqro3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        },
        "outputId": "8e37c9a5-6fa9-42aa-9ea3-0fed1ff21b95"
      },
      "source": [
        "word_to_index_keywords = {word:index for index,word in lda_model.id2word.items()}\n",
        "keywords_seq_master=[]\n",
        "c=0\n",
        "for i in human_sentences:\n",
        "  c=c+1\n",
        "  if(((c/len(human_sentences))*100)%10==0):\n",
        "    print((c/len(human_sentences))*100, \"% complete\")\n",
        "  keywords = get_keywords(lda_model,i,3,5)\n",
        "  keywords_seq = [word_to_index_keywords[key] for key in keywords]\n",
        "  keywords_seq_master.append(keywords_seq)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10.0 % complete\n",
            "20.0 % complete\n",
            "30.0 % complete\n",
            "40.0 % complete\n",
            "50.0 % complete\n",
            "60.0 % complete\n",
            "70.0 % complete\n",
            "80.0 % complete\n",
            "90.0 % complete\n",
            "100.0 % complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iaPGuMSNf55j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d223bd52-f8e0-44f4-a295-db6a37580edd"
      },
      "source": [
        "K_master = embedding_layer_keywords(np.array(keywords_seq_master))\n",
        "print(K_master.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 15, 200)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PgriiK8SgK8G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#creating dataset\n",
        "Kdataset = tf.data.Dataset.from_tensor_slices((K_master))\n",
        "Kdataset = Kdataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LGk3236slTJt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "    'Generates data for Keras'\n",
        "    def __init__(self, batch_size=100, dim=max_length, shuffle=False):\n",
        "        'Initialization'\n",
        "        self.dim = dim\n",
        "        self.batch_size = batch_size\n",
        "        self.len_per_epoch = int(len(X)/self.batch_size)\n",
        "        self.shuffle = shuffle\n",
        "        self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        'Denotes the number of batches per epoch'\n",
        "        return self.len_per_epoch\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        \n",
        "        p, q = self.__data_generation()\n",
        "\n",
        "        return p, q\n",
        "\n",
        "    def __data_generation(self):\n",
        "        \n",
        "        X = np.empty((self.batch_size, self.dim))\n",
        "        y = np.empty((self.batch_size, self.dim))\n",
        "        s0 = np.zeros((self.batch_size, n_s))\n",
        "        c0 = np.zeros((self.batch_size, n_s))\n",
        "        X, y = next(iter(dataset))\n",
        "        y = tf.transpose(y)\n",
        "        K = next(iter(Kdataset))\n",
        "\n",
        "        return [X,s0,c0,K], y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xi8pn-RZJ3Mo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embedding_dim=200\n",
        "Embedding_matrix = np.zeros((len(human_vocab)+1, embedding_dim))\n",
        "for word, i in human_vocab.items():\n",
        "    Embedding_vector = embeddings_index.get(word)\n",
        "    if Embedding_vector is not None:\n",
        "        # words not found in embedding index will be all-zeros.\n",
        "        Embedding_matrix[i] = Embedding_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QShnjewCKSdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "embeddingLayer = tf.keras.layers.Embedding((len(human_vocab)+1),embedding_dim, weights=[Embedding_matrix], trainable=False, input_length=max_length)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrXB-BAUA2Ef",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "repeator = RepeatVector(Tx)\n",
        "concatenator = Concatenate(axis=-1)\n",
        "densor = Dense(1, activation = \"relu\")\n",
        "dotor = Dot(axes = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y9gv7cw1LaAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention(a, s_prev):\n",
        "    \n",
        "    s_prev = repeator(s_prev)\n",
        "    \n",
        "    concat = concatenator([a, s_prev])\n",
        "    \n",
        "    e = densor(concat)\n",
        "    \n",
        "    alphas = tf.nn.softmax(e, axis=1)\n",
        "    \n",
        "    context = dotor([alphas, a])\n",
        "    \n",
        "    return context"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nu0L4QAvFrCE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "post_activation_LSTM_cell = LSTM(n_s, return_state = True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZCq-yVgo206A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gensim\n",
        "import pandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Terl8nMpHGSJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "8d82cf65-eb9b-4021-be75-10769fe42b72"
      },
      "source": [
        "!unzip non_mallet_lda_model_saved_through_gensim-20200706T073922Z-001.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  non_mallet_lda_model_saved_through_gensim-20200706T073922Z-001.zip\n",
            "  inflating: non_mallet_lda_model_saved_through_gensim/lda_non_mallet_model  \n",
            "  inflating: non_mallet_lda_model_saved_through_gensim/lda_non_mallet_model.id2word  \n",
            "  inflating: non_mallet_lda_model_saved_through_gensim/lda_non_mallet_model.expElogbeta.npy  \n",
            "  inflating: non_mallet_lda_model_saved_through_gensim/lda_non_mallet_model.state  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3LZLtij1tT6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "657f939b-de38-4d7a-fbf0-2222937143ee"
      },
      "source": [
        "lda_model = gensim.models.ldamodel.LdaModel.load('model/lda_non_mallet_model')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:253: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
            "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43rg1HyhOWny",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_keywords(lda_model,sentence,take_topics=3,keywords_per_topic = 10): #for eg:returns the list of top 10 keywords from top 3 topics\n",
        "  text = sentence.split()\n",
        "  corpus = lda_model.id2word.doc2bow(text)#converting to doc2bow format the original sentence\n",
        "\n",
        "  prob = lda_model.get_document_topics(corpus) #probability distribution\n",
        "  #print(prob)\n",
        "  prob_sorted = sorted(prob,key = lambda x: x[1],reverse=True)#topics sorted in descending order\n",
        "  #print(prob_sorted)\n",
        "  all_keywords = []\n",
        "  for i,topic in enumerate(prob_sorted):\n",
        "    list = lda_model.print_topic(topic[0],keywords_per_topic).split(\" + \")\n",
        "    keywords = [str.split(\"*\")[1].replace('\"','') for str in list]\n",
        "    all_keywords+=keywords\n",
        "    if(i==take_topics-1):\n",
        "      break\n",
        "  return all_keywords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-gkKzlMK5hK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCjGtRrnLOyT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# these two can be tuned and should be changed accordingly as the input to get_keywords function is changed\n",
        "take_topics = 3\n",
        "keywords_per_topic = 5\n",
        "\n",
        "# Defined shared layers as global variables\n",
        "repeator_topic = RepeatVector(take_topics*keywords_per_topic)  #repeat for total number of topics\n",
        "concatenator = Concatenate(axis=-1)\n",
        "dotor = Dot(axes = 1)\n",
        "\n",
        "\n",
        "#layers for topic attention\n",
        "densor1_topic = Dense(10, activation = \"tanh\")\n",
        "densor2_topic = Dense(1, activation = \"softmax\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6jTf7Wt8LSNP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def one_step_attention_topic(a_last, s_prev, K): #a_last is last cell state , K is the list of embeddings of keywords\n",
        "    #calculating context vector\n",
        "    \n",
        "\n",
        "    #repeating s_prev and a_last for len(K) times\n",
        "\n",
        "    s_prev_temp = repeator_topic(s_prev)\n",
        "    print(\"a:\",a_last.shape,\"\\n\",\"s:\",s_prev_temp.shape,\"\\n\",\"K:\",K.shape)    \n",
        "\n",
        "\n",
        "    concat = concatenator([a_last,s_prev_temp,K])\n",
        "    e = densor1(concat)\n",
        "    alphas = densor2(e)\n",
        "    topic_vector = dotor([alphas,K])\n",
        "    \n",
        "    return topic_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6iE5J-DvDYV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def include_yhat_and_topic(context,out):\n",
        "  reduced = [tf.math.argmax(out,axis=1)]\n",
        "  reduced = tf.convert_to_tensor(reduced)\n",
        "  reduced = tf.transpose(reduced)\n",
        "  reduced = tf.cast(reduced, tf.dtypes.float32)\n",
        "  concatable = RepeatVector(1)(reduced)\n",
        "  concat = Concatenate(axis=-1)([context,concatable])\n",
        "  feedable = Dense(n_s, activation='relu')(concat)\n",
        "  return feedable\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nSBOmec1G29j",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model(Tx, Ty, n_a, n_s, human_vocab_size, machine_vocab_size):\n",
        "    \n",
        "    X = Input(shape=(Tx,))\n",
        "    s0 = Input(shape=(n_s,), name='s0')\n",
        "    c0 = Input(shape=(n_s,), name='c0')\n",
        "    s = s0\n",
        "    c = c0\n",
        "    K_master = Input(shape=(15,200,))\n",
        "    \n",
        "    outputs = []\n",
        "\n",
        "    x = embeddingLayer(X)\n",
        "    \n",
        "    # Define pre-attention Bi-LSTM\n",
        "    a = Bidirectional(LSTM(n_a, return_sequences=True))(x)\n",
        "    \n",
        "    # Iterate for Ty steps\n",
        "    for t in range(Ty):\n",
        "    \n",
        "        # Create context\n",
        "        context = one_step_attention(a, s)\n",
        "        print(s.shape)\n",
        "        def fn(K):\n",
        "          one_step_attention_topic(a, s, K)\n",
        "        @tf.function\n",
        "        def func(tensor):\n",
        "          return tf.map_fn(fn, tensor)\n",
        "\n",
        "        \n",
        "        topic_vector = func(K_master)\n",
        "        print(\"topic vector shape:\",topic_vector.shape)  \n",
        "\n",
        "        # if t=0:\n",
        "        #   context = include_topic(context, topic_vector)\n",
        "        if t!=0:          \n",
        "          context = include_yhat_and_topic(context, out)\n",
        "        # Apply the post-attention LSTM cell to the \"context\" vector.\n",
        "        s, _, c = post_activation_LSTM_cell(context, initial_state=[s, c])\n",
        "        \n",
        "        # Apply Dense layer to the hidden state output of the post-attention LSTM\n",
        "        p = Dense(machine_vocab_size)(s)\n",
        "\n",
        "        out = tf.nn.softmax(p,axis=1)\n",
        "        print(\"this point was reached\")\n",
        "      \n",
        "        \n",
        "        # Append \"out\" to the \"outputs\" list and covert it into tf tensor later\n",
        "        outputs.append(out)\n",
        "    \n",
        "    outputs = tf.convert_to_tensor(outputs)\n",
        "\n",
        "    # Create model instance taking three inputs and returning the tensor of outputs\n",
        "    model = Model([X, s0, c0], outputs)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uIudXpVIoYDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = model(Tx,Ty,n_a,n_s,len(human_vocab),len(machine_vocab))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wP8Vzsfcs2D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=0.0025)\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer=opt)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXrWZwsDfI-2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "8ce2bb4e-9a9c-4add-a14c-d142c28e3313"
      },
      "source": [
        "generator = DataGenerator()\n",
        "model.fit_generator(generator=generator, epochs=7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-83-37fad863d5dc>:2: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.fit, which supports generators.\n",
            "Epoch 1/7\n",
            "500/500 [==============================] - 81s 163ms/step - loss: 1.1150\n",
            "Epoch 2/7\n",
            "500/500 [==============================] - 81s 163ms/step - loss: 0.4382\n",
            "Epoch 3/7\n",
            "500/500 [==============================] - 81s 162ms/step - loss: 0.0369\n",
            "Epoch 4/7\n",
            "500/500 [==============================] - 81s 162ms/step - loss: 0.0104\n",
            "Epoch 5/7\n",
            "500/500 [==============================] - 81s 162ms/step - loss: 0.0073\n",
            "Epoch 6/7\n",
            "500/500 [==============================] - 81s 161ms/step - loss: 0.0064\n",
            "Epoch 7/7\n",
            "500/500 [==============================] - 81s 162ms/step - loss: 0.0059\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1ace3cb780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWCdqVsaXcX1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "outputId": "4ffac719-81f4-4dd4-b0a1-986e15896890"
      },
      "source": [
        "Examples = ['What are you doing?','Hey','Who are you?','Hope you are having a nice day']\n",
        "sequenceE = tokenizer.texts_to_sequences(Examples)\n",
        "X = tf.keras.preprocessing.sequence.pad_sequences(sequences=sequenceE, maxlen=max_length, padding='post', truncating = \"post\")\n",
        "print(X)\n",
        "print(X.shape)\n",
        "s0 = np.zeros((len(Examples),n_s))\n",
        "c0 = np.zeros((len(Examples),n_s))\n",
        "prediction = model.predict([X,s0,c0])\n",
        "perm = [1,0,2]\n",
        "prediction = tf.transpose(prediction, perm=perm)\n",
        "responses = []\n",
        "prediction = np.array(prediction)\n",
        "\n",
        "for i in range(0,prediction.shape[0]):\n",
        "  temp = []\n",
        "  for j in range(0, prediction.shape[1]):\n",
        "    p = np.argmax(prediction[i][j])\n",
        "    temp.append(p)\n",
        "  responses.append(temp)\n",
        "\n",
        "final_responses = []\n",
        "print(responses)\n",
        "for k in range(0, len(responses)):\n",
        "  tempo = []\n",
        "  for l in range(0,len(responses[k])):\n",
        "    if responses[k][l]!=0:\n",
        "      kkk = reverse_machine_vocab[responses[k][l]]\n",
        "      tempo.append(kkk)\n",
        "  final_responses.append(tempo)\n",
        "print(\"\\n\")\n",
        "for m in range(0,len(final_responses)):\n",
        "  print(Examples[m],\"\\n\")\n",
        "  print(\"-->\",' '.join(final_responses[m]),\"\\n\\n\") \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 14  33   2 257   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [161   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [ 82  33   2   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [359   2  33 634   6 226 193   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0   0   0]]\n",
            "(4, 30)\n",
            "[[382, 7, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [9, 28, 305, 477, 17, 983, 883, 264, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [43, 24, 47, 679, 100, 4730, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [4, 47, 185, 3, 53, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n",
            "\n",
            "\n",
            "What are you doing? \n",
            "\n",
            "--> under it \n",
            "\n",
            "\n",
            "Hey \n",
            "\n",
            "--> and don't forget cut your date short tonight \n",
            "\n",
            "\n",
            "Who are you? \n",
            "\n",
            "--> come on can brought some swinging \n",
            "\n",
            "\n",
            "Hope you are having a nice day \n",
            "\n",
            "--> i can hear the right you \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}